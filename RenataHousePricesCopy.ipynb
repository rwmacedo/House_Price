{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d92ab48-7e1d-43ad-99d4-4d8cb352905c",
   "metadata": {
    "id": "ffb60bca-8525-473c-a542-238da6fc9057",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#import re\n",
    "import time\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api_pb2\n",
    "from tensorboard.plugins.hparams import summary as hparams_summary\n",
    "%load_ext tensorboard\n",
    "\n",
    "from google.protobuf import struct_pb2\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format # show only two digits\n",
    "pd.set_option('display.max_columns', 100) # show up to 100 columns\n",
    "np.random.seed(2023)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.head()\n",
    "\n",
    "Verificando o tamanho das bases\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "Utilizando a coluna Id como índice\n",
    "\n",
    "train.set_index(\"Id\", inplace=True)\n",
    "test.set_index(\"Id\", inplace=True)\n",
    "\n",
    "Verificar se há linhas duplicadas\n",
    "\n",
    "linhas_duplicadas = train[train.duplicated()]\n",
    "\n",
    "if len(linhas_duplicadas) > 0:\n",
    "    print(\"Linhas duplicadas encontradas:\")\n",
    "    print(linhas_duplicadas)\n",
    "else:\n",
    "    print(\"Não há linhas duplicadas no DataFrame.\")\n",
    "\n",
    "# Remover linhas duplicadas\n",
    "#df_sem_duplicatas = df.drop_duplicates()\n",
    "\n",
    "\n",
    "train.info()\n",
    "\n",
    "# Calculando a mediana nas variáveis numéricas na base de treino\n",
    "median_values = train.select_dtypes(exclude=['object']).median()\n",
    "\n",
    "# Imputando a mediana nas variáveis numéricas em train, mantendo o índice\n",
    "dados_numerical = train.select_dtypes(exclude=['object']).fillna(median_values)\n",
    "\n",
    "# Tratamento das variáveis categóricas em train, mantendo o índice\n",
    "dados_categoric = train.select_dtypes(include=['object']).fillna('NONE')\n",
    "\n",
    "# Combina os dados numéricos e categóricos tratados, garantindo a ordem original das linhas\n",
    "# Aqui, você não precisa fazer um merge já que você está tratando separadamente e quer combinar de volta\n",
    "train = pd.concat([dados_numerical, dados_categoric], axis=1)\n",
    "\n",
    "# Imputando a mediana nas variáveis numéricas em test com a mediana de train\n",
    "dados_numerical_test = test.select_dtypes(exclude=['object']).fillna(median_values)\n",
    "\n",
    "# Tratamento das variáveis categóricas em test, mantendo o índice\n",
    "dados_categoric_test = test.select_dtypes(include=['object']).fillna('NONE')\n",
    "\n",
    "# Combina os dados numéricos e categóricos tratados de test, garantindo a ordem original das linhas\n",
    "test = pd.concat([dados_numerical_test, dados_categoric_test], axis=1)\n",
    "\n",
    "\n",
    "# a variável MSSubClass aparece como int, mas é categ+orica\n",
    "train['MSSubClass'] = train['MSSubClass'].astype(str)\n",
    "test['MSSubClass'] = test['MSSubClass'].astype(str)\n",
    "\n",
    "\n",
    "test.info()\n",
    "\n",
    "\n",
    "variaveis = (\"MSSubClass\", \"LotArea\", \"LotShape\", \"LandContour\", \"LotConfig\", \"LandSlope\", \"BldgType\", \"1stFlrSF\", \"2ndFlrSF\", \"BedroomAbvGr\", \"FullBath\", \"HalfBath\", \"Kitchen\", \"TotRmsAbvGrd\", \"Heating\", \"Electrical\", \"Utilities\", \"CentralAir\", \"YrSold\", \"SaleType\", \"SaleCondition\", \"MSZoning\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"YearBuilt\", \"YearRemodAdd\", \"OverallQual\", \"OverallCond\", \"ExterQual\", \"ExterCond\", \"HouseStyle\", \"KitchenQual\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \"FireplaceQu\", \"Functional\", \"GarageType\", \"GarageFinish\", \"GarageArea\", \"PoolArea\", \"Fence\", \"MiscFeature\", \"Fireplaces\", \"TotalBsmtSF\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"SalePrice\")\n",
    "\n",
    "dados_train = train[list(filter(lambda x: x in train.columns, variaveis))]\n",
    "dados_test = test[list(filter(lambda x: x in test.columns, variaveis))]\n",
    "\n",
    "train = dados_train\n",
    "test = dados_test\n",
    "\n",
    "# Displaying the shapes of the new datasets\n",
    "print(\"Train dataset shape:\", train.shape)\n",
    "print(\"Test dataset shape:\", test.shape)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "#### Análise univariada\n",
    "\n",
    "train.tail()\n",
    "\n",
    "train['CentralAir'].unique()\n",
    "\n",
    "train['CentralAir'] = train['CentralAir'].replace({'Y': 1, 'N': 0})\n",
    "test['CentralAir'] = test['CentralAir'].replace({'Y': 1, 'N': 0})\n",
    "\n",
    "cols = train.columns\n",
    "\n",
    "categoricas_nominais= ['MSSubClass', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2','HouseStyle', 'Functional', 'GarageType', 'GarageFinish',\n",
    "'SaleType', 'SaleCondition','BldgType', 'Heating','Electrical', 'MSZoning',\"MiscFeature\",'YrSold','YearBuilt', 'YearRemodAdd']\n",
    "\n",
    "categoricas_ordinais1 = ['OverallQual', 'OverallCond']\n",
    "categoricas_ordinais2 = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'Fence']\n",
    "#categoricas_ordinais3 = ['YrSold','YearBuilt', 'YearRemodAdd']\n",
    "\n",
    "\n",
    "colunas_numericas = train.drop(columns=categoricas_nominais + categoricas_ordinais1 + categoricas_ordinais2).columns\n",
    "df_numericos = train[colunas_numericas]\n",
    "colunas_numericas\n",
    "\n",
    "for col in colunas_numericas:\n",
    "    plt.figure(figsize=(18,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    #sns.distplot(df[col])\n",
    "    sns.histplot(train[col], kde=True)\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxplot(x=col, data=train)\n",
    "    plt.show()\n",
    "\n",
    "#Variaveis nominais\n",
    "for col in categoricas_nominais:\n",
    "    train[col].value_counts().plot(kind=\"bar\", figsize=(5,3))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Variaveis nominais\n",
    "for col in categoricas_ordinais1:\n",
    "    train[col].value_counts().plot(kind=\"bar\", figsize=(5,3))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for col in categoricas_ordinais2:\n",
    "    train[col].value_counts().plot(kind=\"bar\", figsize=(5,3))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#### Análise bivaliada\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.pairplot(train[colunas_numericas].select_dtypes(include='number'))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(train[colunas_numericas].corr(), cbar=True, annot=True, cmap='Blues')\n",
    "\n",
    "\n",
    "\n",
    "# codificando categoricas_ordinais1 da base treino\n",
    "dados_ord1 = train[categoricas_ordinais1]\n",
    "dados_ord1.index = range(1, len(train)+1)\n",
    "\n",
    "# codificando categoricas_ordinais1 da base teste\n",
    "dados_ord1_test = test[categoricas_ordinais1]\n",
    "dados_ord1_test.index = range(1, len(test)+1)\n",
    "\n",
    "\n",
    "\n",
    "print(dados_ord1.shape)\n",
    "print(dados_ord1_test.shape)\n",
    "\n",
    "# codificando categoricas_ordinais2 da base treino\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dados_ord2 = train[categoricas_ordinais2]\n",
    "\n",
    "# Mapeamento da ordem correta\n",
    "ordem_correta = {'Ex': 0, 'Gd': 1, 'TA': 2, 'Fa': 3, 'Po': 4, 'NA': 5}\n",
    "\n",
    "# Criando um objeto para transformar as colunas categóricas usando LabelEncoder\n",
    "cat_encoder = LabelEncoder()\n",
    "\n",
    "# Aplicando o LabelEncoder em cada coluna categórica\n",
    "for coluna in dados_ord2.columns:\n",
    "    dados_ord2[coluna] = dados_ord2[coluna].map(ordem_correta)\n",
    "    dados_ord2[coluna] = cat_encoder.fit_transform(dados_ord2[coluna])\n",
    "\n",
    "dados_ord2.index = range(1, len(train)+1)\n",
    "\n",
    "\n",
    "\n",
    "# codificando categoricas_ordinais2 da base teste\n",
    "\n",
    "dados_ord2_test = test[categoricas_ordinais2]\n",
    "\n",
    "# Mapeamento da ordem correta\n",
    "ordem_correta = {'Ex': 0, 'Gd': 1, 'TA': 2, 'Fa': 3, 'Po': 4, 'NA': 5}\n",
    "\n",
    "# Criando um objeto para transformar as colunas categóricas usando LabelEncoder\n",
    "cat_encoder = LabelEncoder()\n",
    "\n",
    "# Aplicando o LabelEncoder em cada coluna categórica\n",
    "for coluna in dados_ord2.columns:\n",
    "    dados_ord2_test[coluna] = dados_ord2_test[coluna].map(ordem_correta)\n",
    "    dados_ord2_test[coluna] = cat_encoder.fit_transform(dados_ord2_test[coluna])\n",
    "\n",
    "dados_ord2_test.index = range(1, len(test)+1)\n",
    "\n",
    "print(dados_ord1.shape)\n",
    "print(dados_ord1_test.shape)\n",
    "\n",
    "# Mostrando o resultado\n",
    "print(dados_ord2_test)\n",
    "\n",
    "categoricas_nominais\n",
    "\n",
    "# Tratando variáveis nominais na base treino\n",
    "\n",
    "dados_nom = train[categoricas_nominais]\n",
    "dados_nom.head()\n",
    "\n",
    "# Tratando variáveis nominais na base treino\n",
    "dados_nom_test = test[categoricas_nominais]\n",
    "dados_nom_test.head()\n",
    "\n",
    "dados_nom = dados_nom.T.drop_duplicates().T\n",
    "dados_nom_test = dados_nom_test.T.drop_duplicates().T\n",
    "\n",
    "\n",
    "dados_nom_test.info()\n",
    "\n",
    "#Checking for wrong entries like symbols -,?,#,*,etc.\n",
    "for col in dados_nom_test.columns:\n",
    "    print('{} : {}'.format(col, dados_nom_test[col].unique()))\n",
    "\n",
    "#treino\n",
    "# Criar um objeto OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "\n",
    "# Aplicar o OneHotEncoder nas colunas categóricas nominais\n",
    "dados_categoricas_nominais_one_hot_encoded = onehot_encoder.fit_transform(train[categoricas_nominais])\n",
    "\n",
    "# Obter os nomes das colunas a partir do objeto OneHotEncoder\n",
    "colunas = onehot_encoder.get_feature_names_out()\n",
    "\n",
    "# Transformar o objeto numpy.ndarray em um DataFrame\n",
    "dados_nom = pd.DataFrame(dados_categoricas_nominais_one_hot_encoded, columns=colunas)\n",
    "\n",
    "dados_nom.index = range(1, len(train)+1)\n",
    "\n",
    "dados_nom\n",
    "\n",
    "\n",
    "#treino\n",
    "# Criar um objeto OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "\n",
    "# Aplicar o OneHotEncoder nas colunas categóricas nominais\n",
    "dados_categoricas_nominais_one_hot_encoded_test = onehot_encoder.fit_transform(test[categoricas_nominais])\n",
    "\n",
    "# Obter os nomes das colunas a partir do objeto OneHotEncoder\n",
    "colunas = onehot_encoder.get_feature_names_out()\n",
    "\n",
    "# Transformar o objeto numpy.ndarray em um DataFrame\n",
    "dados_nom_test = pd.DataFrame(dados_categoricas_nominais_one_hot_encoded_test, columns=colunas)\n",
    "\n",
    "dados_nom_test.index = range(1, len(test)+1)\n",
    "\n",
    "dados_nom_test\n",
    "\n",
    "print(dados_nom.shape)\n",
    "print(dados_nom_test.shape)\n",
    "\n",
    "def encontrar_colunas_diferentes(df1, df2):\n",
    "    colunas_df1 = set(df1.columns)\n",
    "    colunas_df2 = set(df2.columns)\n",
    "    \n",
    "    colunas_diferentes = colunas_df1.symmetric_difference(colunas_df2)\n",
    "    \n",
    "    return colunas_diferentes\n",
    "colunas_diferentes = encontrar_colunas_diferentes(dados_nom, dados_nom_test)\n",
    "\n",
    "print(\"Colunas diferentes nos dois DataFrames:\")\n",
    "print(colunas_diferentes)\n",
    "\n",
    "colunas_diferentes\n",
    "\n",
    "dummy_names = dados_nom.columns.tolist()\n",
    "dummy_names_test = dados_nom_test.columns.tolist()\n",
    "\n",
    "# Printing the column names\n",
    "print(dummy_names)\n",
    "\n",
    "train.info()\n",
    "\n",
    "Padronizando as variáveis numéricas \n",
    "\n",
    "# Standardizar\n",
    "#treino\n",
    "def normalize_column(col):\n",
    "    return (col - col.mean()) / col.std() if col.dtype in ['int64', 'float64','int32'] else col\n",
    "\n",
    "# Apply the function to each column\n",
    "normalized_data = train.apply(normalize_column)\n",
    "\n",
    "#test\n",
    "def normalize_column(col):\n",
    "    return (col - col.mean()) / col.std() if col.dtype in ['int64', 'float64','int32'] else col\n",
    "\n",
    "# Apply the function to each column\n",
    "normalized_data_test = test.apply(normalize_column)\n",
    "\n",
    "# Viewing the resulting DataFrame\n",
    "normalized_data.head()\n",
    "\n",
    "print(normalized_data.shape)\n",
    "print(normalized_data_test.shape)\n",
    "\n",
    "\n",
    "dados_numericos_padronizados = normalized_data.select_dtypes(exclude=['object'])\n",
    "\n",
    "dados_numericos_padronizados_test = normalized_data_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "\n",
    "dados_numericos_padronizados.index = range(1, len(train)+1)\n",
    "dados_numericos_padronizados_test.index = range(1, len(test)+1)\n",
    "\n",
    "print(dados_numericos_padronizados.shape)\n",
    "print(dados_numericos_padronizados_test.shape)\n",
    "\n",
    "# Juntando as tabelas\n",
    "\n",
    "train = pd.concat([dados_numericos_padronizados, dados_nom, dados_ord1, dados_ord2], axis=1)\n",
    "\n",
    "test = pd.concat([dados_numericos_padronizados_test, dados_nom_test, dados_ord1_test, dados_ord2_test], axis=1)\n",
    "\n",
    "\n",
    "train.head()\n",
    "\n",
    "# Displaying the shapes of the new datasets\n",
    "print(\"Train dataset shape:\", train.shape)\n",
    "print(\"Test dataset shape:\", test.shape)\n",
    "\n",
    "#train= train.drop(columns=\"MSSubClass\")\n",
    "#test= test.drop(columns=\"MSSubClass\")\n",
    "\n",
    "Vamos fazer o test VIF, para isso, vamos separar uma base de dados sem as variáveis dummies e sem a variável alvo\n",
    "\n",
    "df_first_19 = train.iloc[:, :22].copy()\n",
    "df_last_10 = train.iloc[:, -10:].copy()\n",
    "\n",
    "save_my_sale = train.iloc[:, 22].copy()\n",
    "\n",
    "#my_template = train\n",
    "train_SD = pd.concat([df_first_19, df_last_10], axis=1)\n",
    "\n",
    "print(save_my_sale)\n",
    "#print(dados)\n",
    "\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "independent_vars = train_SD\n",
    "\n",
    "# Compute VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = independent_vars.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "\n",
    "print(vif_data)\n",
    "\n",
    "max_value = vif_data['VIF'].max()\n",
    "\n",
    "print(f\"The maximum value in the column is: {max_value}\")\n",
    "\n",
    "train.info()\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Assuming 'train' is your DataFrame containing predictor variables initially\n",
    "independent_vars = train_SD.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "\n",
    "high_vif_variables = pd.DataFrame({'Variable': [], 'VIF': []})\n",
    "\n",
    "while True:\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = independent_vars.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "    high_vif_variables = vif_data[vif_data['VIF'] > 2]\n",
    "\n",
    "    if high_vif_variables.empty:\n",
    "        break\n",
    "\n",
    "    variable_to_drop = high_vif_variables.loc[high_vif_variables['VIF'].idxmax(), 'Variable']\n",
    "\n",
    "    independent_vars.drop(variable_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(\"Variables after dropping high VIF variables:\")\n",
    "print(independent_vars.columns)\n",
    "\n",
    "\n",
    "column_names2 = independent_vars.columns.tolist()\n",
    "\n",
    "# Printing the column names\n",
    "print(column_names2)\n",
    "\n",
    "independent_vars.head()\n",
    "\n",
    "#excluindo Fence\n",
    "dados2 = independent_vars.iloc[:, :-1]\n",
    "dados2.head()\n",
    "\n",
    "#Juntando as variáveis selecionadas pelo VIF com as dummies\n",
    "dados1 = train[dummy_names]\n",
    "#dados2 = train[column_names2]\n",
    "\n",
    "\n",
    "dados3 = pd.concat([dados2, dados1], axis=1)\n",
    "#dados3 = pd.concat([dados2], axis=1)\n",
    "dados3.head()\n",
    "\n",
    "dados3.shape\n",
    "\n",
    "my_df = pd.DataFrame(save_my_sale)\n",
    "my_df.head()\n",
    "\n",
    "#Juntando a variável alvo com a base\n",
    "dados3 = pd.concat([dados3, my_df], axis=1)\n",
    "\n",
    "dados3.head()\n",
    "\n",
    "dados3.tail()\n",
    "\n",
    "#Verificar se ficol algum valor ausente\n",
    "columns_with_nan = dados3.columns[dados3.isnull().any()].tolist()\n",
    "\n",
    "print(\"Columns containing NaN values:\")\n",
    "print(columns_with_nan)\n",
    "\n",
    "#redefinando o nome train para a base de treino com as variáveis selecionadas\n",
    "train = dados3  # Filter rows where 'id' is from 1 to 1460\n",
    "\n",
    "# Displaying the shapes of the new datasets\n",
    "print(\"Train dataset shape:\", train.shape)\n",
    "print(\"Test dataset shape:\", test.shape)\n",
    "\n",
    "#### Vamos fazer uma seleção de variáveis com o modelo lasso\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Separar variáveis independentes (X) e variável dependente (y)\n",
    "X = train.drop('SalePrice', axis=1)\n",
    "y = train['SalePrice']\n",
    "\n",
    "# Dividir os dados em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# Ajustar um modelo Lasso\n",
    "lasso = Lasso(alpha=0.02)  # O valor de alpha controla a força da penalidade L\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Verificar os coeficientes não nulos (variáveis selecionadas)\n",
    "variaveis_selecionadas = X.columns[lasso.coef_ != 0]\n",
    "\n",
    "# Exibir as variáveis selecionadas\n",
    "print(\"Variáveis selecionadas pelo Lasso:\", variaveis_selecionadas)\n",
    "\n",
    "\n",
    "# Separando a variável dependente e independnte\n",
    "#X_train = train.drop('SalePrice', axis=1)\n",
    "\n",
    "X_train = train[variaveis_selecionadas]\n",
    "y_train = train['SalePrice']\n",
    "\n",
    "colunas_treino = X_train.columns\n",
    "colunas_treino\n",
    "\n",
    "\n",
    "X_test = test[colunas_treino]\n",
    "#X_test = test[variaveis_selecionadas]\n",
    "#y_test = test['SalePrice']  # Variável dependente\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "Vamos usar a base de treino para treinar e avaliar o modelo. No final usaremos a base de teste para previsão. Chamaremos a base X_teste de Xproducao, para evitar confusão com o teste do treinamento\n",
    "\n",
    "X_producao = X_test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supondo que `X` são suas features e `y` é o target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Inicializando e treinando o MLPRegressor\n",
    "ANN = MLPRegressor(hidden_layer_sizes=(5, 5, 5), activation='relu', random_state=2023)\n",
    "ANN.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = ANN.predict(X_test)\n",
    "\n",
    "# Calculando o erro médio quadrático\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculando o coeficiente de determinação R²\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Definindo o modelo\n",
    "mlp = MLPRegressor(max_iter=1000)\n",
    "\n",
    "# Definindo os hiperparâmetros para teste\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,), (10,10), (50,50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "# Usando r2_score como a métrica de avaliação\n",
    "scorer = make_scorer(r2_score)\n",
    "\n",
    "# Configurando GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5, scoring=scorer)\n",
    "\n",
    "# Fit no modelo (isso pode demorar um pouco dependendo do número de combinações)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhor parâmetro encontrado\n",
    "print(\"Melhor conjunto de parâmetros:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Melhor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Avaliando o melhor modelo no conjunto de teste\n",
    "y_pred = best_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² do melhor modelo no conjunto de teste: {r2}')\n",
    "\n",
    "\n",
    "import pickle\n",
    "# Salvando o modelo em um arquivo pickle\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score## Comparando com Randon Forest\n",
    "\n",
    "# Criar um modelo de Random Forest Regressor\n",
    "\n",
    "modelo_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "previsoes = modelo_rf.predict(X_test)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "mse = mean_squared_error(y_test, previsoes)\n",
    "r2 = r2_score(y_test, previsoes)\n",
    "\n",
    "print(f'Erro Médio Quadrático (MSE): {mse:.2f}')\n",
    "print(f'R²: {r2:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
